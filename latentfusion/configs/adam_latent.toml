type = "gradient"

[args]
  optimizer = "adam"
  num_iters = 150
  num_samples = 16
  ranking_size = 16
  learning_rate = 0.01
  lr_reduce_patience = 10
  lr_reduce_threshold = 1e-5
  converge_threshold = 1e-6
  converge_patience = 25


[loss_weights]
  depth = 1.0
  ov_depth = 0.3
  iou = 0.0
  mask = 0.0
  latent = 0.2
