type = "gradient"

[args]
  optimizer = "adam"
  num_iters = 100
  num_samples = 8
  ranking_size = 8
  learning_rate = 0.01
  lr_reduce_patience = 10
  lr_reduce_threshold = 1e-4
  converge_threshold = 1e-6
  converge_patience = 10


[loss_weights]
  depth = 1.0
  ov_depth = 0.3
  iou = 0.0
  mask = 0.0
  latent = 0.0
