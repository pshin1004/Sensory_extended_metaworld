{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.model_zoo\n",
    "\n",
    "MOPED_PATH = Path('')\n",
    "LINEMOD_PATH = Path('')\n",
    "CHECKPOINT = Path('')\n",
    "\n",
    "num_input_views = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentfusion.recon.inference import LatentFusionModel\n",
    "\n",
    "model = LatentFusionModel.from_checkpoint(checkpoint, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentfusion.recon.inference import Observation\n",
    "from latentfusion.pose import bop\n",
    "from latentfusion.datasets.bop import BOPDataset\n",
    "from latentfusion.datasets.realsense import RealsenseDataset\n",
    "import latentfusion.visualization as viz\n",
    "from latentfusion.augment import gan_denormalize\n",
    "from latentfusion import meshutils\n",
    "from latentfusion import augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LINEMOD\n",
    "\n",
    "Uncomment this and comment out the MOPED cell to use LINEMOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_id = 4\n",
    "# frame = 0\n",
    "# input_scene_path = LINEMOD_PATH / f'train/{object_id:06d}'\n",
    "# target_scene_path = LINEMOD_PATH / f'test/{object_id:06d}'\n",
    "\n",
    "# input_dataset = BOPDataset(LINEMOD_PATH, input_scene_path, object_id=object_id, object_scale=None)\n",
    "# print('object_scale', input_dataset.object_scale)\n",
    "# target_dataset = BOPDataset(LINEMOD_PATH, target_scene_path, object_id=object_id, object_scale=None)\n",
    "# object_scale_to_meters = 1.0 / (1000.0 * target_dataset.object_scale)\n",
    "# pointcloud = input_dataset.load_pointcloud()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object_id = 'toy_plane'\n",
    "frame = 100\n",
    "\n",
    "input_scene_dir = MOPED_PATH / object_id / 'reference'\n",
    "target_scene_dir = MOPED_PATH / object_id / 'evaluation'\n",
    "\n",
    "pointcloud_path = input_scene_dir / 'integrated_registered_processed.obj'\n",
    "obj = meshutils.Object3D(pointcloud_path)\n",
    "pointcloud = torch.tensor(obj.vertices, dtype=torch.float32)\n",
    "diameter = obj.bounding_diameter\n",
    "object_scale = 1.0 / diameter\n",
    "object_scale_to_meters = 1.0 / object_scale\n",
    "\n",
    "input_paths = [x for x in input_scene_dir.iterdir() if x.is_dir()]\n",
    "input_dataset = RealsenseDataset(input_paths,\n",
    "                                 image_scale=1.0,\n",
    "                                 object_scale=object_scale,\n",
    "                                 odometry_type='open3d')\n",
    "target_paths = sorted([x for x in target_scene_dir.iterdir() if x.is_dir()])\n",
    "target_dataset = RealsenseDataset(target_paths,\n",
    "                                  image_scale=1.0,\n",
    "                                  object_scale=object_scale,\n",
    "                                  odometry_type='open3d',\n",
    "                                  use_registration=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Dataset into Observations\n",
    "\n",
    "Here we load the data into the `Observation` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_obs = Observation.from_dataset(input_dataset, inds=input_dataset.sample_evenly(num_input_views))\n",
    "target_obs = Observation.from_dataset(target_dataset, inds=list(range(len(target_dataset)))[frame:frame+1])\n",
    "\n",
    "input_obs_pp = model.preprocess_observation(input_obs)\n",
    "input_obs_pp_gt = model.preprocess_observation(input_obs)\n",
    "target_obs_pp = model.preprocess_observation(target_obs)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(gan_denormalize(input_obs_pp.color), nrow=4)\n",
    "plt.subplot(132)\n",
    "viz.show_batch(viz.colorize_depth(input_obs_pp.depth), nrow=4)\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(input_obs_pp.mask), nrow=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Latent Object\n",
    "\n",
    "This builds the 'latent object', referred to as `z_obj` in this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentfusion import three\n",
    "import math\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_obj = model.build_latent_object(input_obs_pp)\n",
    "\n",
    "    # Visualize prediction.\n",
    "    camera = input_obs_pp.camera.clone()\n",
    "    y, z = model.render_latent_object(z_obj, camera.to(device))\n",
    "\n",
    "recon_error = (y['depth'].detach().cpu() - input_obs_pp_gt.depth).abs()\n",
    "print('recon_error', recon_error.mean().item())\n",
    "    \n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(viz.colorize_depth(input_obs_pp.depth), nrow=4, title='GT Depth')\n",
    "plt.subplot(132)\n",
    "viz.show_batch(viz.colorize_depth(y['depth'].detach().cpu()), nrow=4, title='Predicted Depth')\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(recon_error), nrow=4, title='Error')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some novel views\n",
    "\n",
    "This visualizes the object from novel views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentfusion.three.orientation import evenly_distributed_quats\n",
    "num_novel = 16\n",
    "camera_ref = input_obs.camera[0]\n",
    "camera_ref = camera_ref.zoom(None, model.camera_dist, model.input_size)\n",
    "quats = evenly_distributed_quats(num_novel, upright=True).to(device)\n",
    "camera_test = camera_ref.clone().repeat(num_novel)\n",
    "camera_test.quaternion = quats\n",
    "\n",
    "with torch.no_grad():\n",
    "    y, _ = model.render_ibr_basic(z_obj, input_obs, camera_test.clone().to(device), apply_mask=True)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(viz.colorize_depth(y['depth']), nrow=4, title='Novel View Depth')\n",
    "plt.subplot(132)\n",
    "viz.show_batch(gan_denormalize(y['color'].cpu()), nrow=4, title='Novel View Color (IBR)')\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(y['mask']), nrow=4, title='Novel View Mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import latentfusion.pose.estimation as pe\n",
    "from functools import partial\n",
    "\n",
    "estimator = pe.load_from_config('../configs/cross_entropy_latent.toml', model, return_camera_history=False, verbose=False)\n",
    "\n",
    "coarse_camera = estimator.estimate(z_obj, target_obs[0])\n",
    "camera_zoom = coarse_camera.zoom(None, model.camera_dist, model.input_size)\n",
    "\n",
    "# Visualize prediction.\n",
    "pred_y, pred_z = model.render_latent_object(z_obj, camera_zoom.to(device))\n",
    "pred_mask = pred_y['mask'].squeeze(0)\n",
    "pred_depth = pred_y['depth'].squeeze(0)\n",
    "pred_depth = camera_zoom.denormalize_depth(pred_depth) * pred_mask\n",
    "pred_depth, _ = camera_zoom.uncrop(pred_depth)\n",
    "pred_mask, _ = camera_zoom.uncrop(pred_mask)\n",
    "pred_depth = pred_depth.cpu()\n",
    "pred_mask = pred_mask.cpu()\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(target_obs.color[0], nrow=2)\n",
    "plt.subplot(132)\n",
    "viz.show_batch(viz.colorize_tensor(target_obs[0].depth.cpu(), cmin=pred_depth.max()-1, cmax=pred_depth.max()), nrow=2)\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(target_obs.prepare()[0].mask.cpu()), nrow=2)\n",
    "\n",
    "plt.figure(2, figsize=(20, 10))\n",
    "plt.subplot(121)\n",
    "viz.show_batch(viz.colorize_tensor(pred_depth.detach().cpu(), cmin=pred_depth.max()-1, cmax=pred_depth.max()), nrow=4, title=\"Estimated Depth\")\n",
    "\n",
    "plt.subplot(122)\n",
    "viz.show_batch(viz.colorize_tensor((target_obs.prepare()[0].depth - pred_depth.detach().cpu()).abs()), nrow=4, title=\"Estimated Depth L1 Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from latentfusion import utils\n",
    "from latentfusion.pose import utils as pose_utils\n",
    "from latentfusion.modules.geometry import Camera\n",
    "\n",
    "sgd_estimator = pe.load_from_config('../configs/adam_quick.toml', model, track_stats=True, return_camera_history=True, num_iters=100)\n",
    "\n",
    "init_camera = coarse_camera.clone()\n",
    "refined_camera, stat_history, camera_history = sgd_estimator.estimate(z_obj, target_obs[0], camera=init_camera)\n",
    "camera_zoom = refined_camera.zoom(None, model.camera_dist, model.input_size)\n",
    "\n",
    "# Visualize prediction.\n",
    "pred_y, pred_z = model.render_latent_object(z_obj, camera_zoom.to(device))\n",
    "pred_mask = pred_y['mask'].squeeze(0)\n",
    "pred_depth = pred_y['depth'].squeeze(0)\n",
    "pred_depth = camera_zoom.denormalize_depth(pred_depth) * (pred_y['mask'].squeeze(0) > 0.5)\n",
    "pred_depth, _ = camera_zoom.uncrop(pred_depth)\n",
    "pred_mask, _ = camera_zoom.uncrop(pred_mask)\n",
    "depth_error = (target_obs.prepare()[0].depth - pred_depth.detach().cpu()).abs()\n",
    "pred_depth = pred_depth.cpu()\n",
    "pred_mask = pred_mask.cpu()\n",
    "depth_error = depth_error.cpu()\n",
    "\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "plt.subplot(131)\n",
    "viz.show_batch(target_obs.color[0], nrow=2)\n",
    "plt.subplot(132)\n",
    "viz.show_batch(viz.colorize_tensor(target_obs[0].depth, cmin=pred_depth.max()-1, cmax=pred_depth.max()), nrow=2)\n",
    "plt.subplot(133)\n",
    "viz.show_batch(viz.colorize_tensor(target_obs.prepare()[0].mask), nrow=2)\n",
    "\n",
    "plt.figure(2, figsize=(20, 10))\n",
    "plt.subplot(121)\n",
    "viz.show_batch(viz.colorize_tensor(pred_depth.detach().cpu(), cmin=pred_depth.max()-1, cmax=pred_depth.max()), nrow=4, title=\"Estimated Depth\")\n",
    "\n",
    "plt.subplot(122)\n",
    "viz.show_batch(viz.colorize_tensor((target_obs.prepare()[0].depth - pred_depth.detach().cpu()).abs()), nrow=4, title=\"Estimated Depth L1 Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentfusion.pose.metrics import camera_metrics\n",
    "from latentfusion.pose.format import metrics_table_multiple\n",
    "\n",
    "\n",
    "for j in range(1):\n",
    "    print(metrics_table_multiple([\n",
    "            camera_metrics(target_obs.camera, coarse_camera[j], pointcloud, object_scale_to_meters),\n",
    "            camera_metrics(target_obs.camera, refined_camera[j], pointcloud, object_scale_to_meters),\n",
    "    ], ['Coarse', 'Refined'], tablefmt='simple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot pose refinement stats over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "viz.plot_grid(3, figsize=(30, 15), plots=[\n",
    "    viz.Plot('Angular Error', [stat_history['angle_dist']/math.pi*180], \n",
    "             params={'ylabel': 'Error (deg)', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Translation Error', [stat_history['trans_dist']*object_scale_to_meters], \n",
    "             params={'ylabel': 'Error (m)', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Rank Loss', [stat_history['rank_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Optim Loss', [stat_history['optim_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    \n",
    "    viz.Plot('Depth Loss', [stat_history['depth_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Overlap Depth Loss', [stat_history['ov_depth_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('Mask Loss', [stat_history['mask_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('IOU Loss', [stat_history['iou_loss']], \n",
    "             params={'ylabel': 'Loss', 'xlabel': 'Iteration'}),\n",
    "    viz.Plot('<5 deg <5 cm', [(stat_history['trans_dist']*object_scale_to_meters < 0.05) & (stat_history['angle_dist']/math.pi*180 < 5)], \n",
    "             params={'ylabel': 'Success', 'xlabel': 'Iteration'}),\n",
    "]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
